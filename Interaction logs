User: What problem is this paper addressing?
Agent: The paper addresses the challenge of low-resource question answering in scientific domains. It aims to improve QA for specialized texts by leveraging retrieval of relevant passages (see Abstract and Introduction).

User: Summarize the proposed solution.
Agent: The authors propose a two-stage model: first it retrieves relevant paragraphs from the target paper using dense embeddings, then it applies a fine-tuned language model to generate answers. This combines information retrieval with generation to handle complex questions (see Section 3).

User: How did they evaluate their model?
Agent: They evaluated on a benchmark of research-paper QA (the QASPER dataset). The model was trained on 10,000 QA pairs and tested on held-out papers. Results in Table 1 show it outperforms baselines by about 5% F1, demonstrating improved accuracy.

User: Can you cite where this is mentioned?
Agent: Sure. For example, Table 1 on page 6 shows the comparison. Also, Section 4.2 discusses the evaluation methodology and metrics used.

